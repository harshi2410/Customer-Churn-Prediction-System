# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbrpOIAmngeOkb-xqutCy6mIU809qMpE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv("https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv")

print("Dataset Shape:", df.shape)
print("\nFirst 3 rows:")
print(df.head(3))
print("\nColumn names:")
print(df.columns.tolist())
print("\nTarget variable distribution:")
print(df['Churn'].value_counts())
print("\nChurn percentage: {:.2f}%".format((df['Churn'].value_counts()['Yes'] / len(df)) * 100))

# Visualizing
plt.figure(figsize=(8, 5))
sns.countplot(x='Churn', data=df, palette='Set2')
plt.title('Customer Churn Distribution')
plt.ylabel('Number of Customers')
plt.show()

# Churn by gender
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
sns.countplot(x='gender', hue='Churn', data=df, palette='Set2')
plt.title('Churn by Gender')

# Churn by contract type
plt.subplot(1, 2, 2)
sns.countplot(x='Contract', hue='Churn', data=df, palette='Set2')
plt.title('Churn by Contract Type')
plt.tight_layout()
plt.show()

# Monthly charges distribution
plt.figure(figsize=(10, 5))
sns.boxplot(x='Churn', y='MonthlyCharges', data=df, palette='Set2')
plt.title('Monthly Charges vs Churn')
plt.show()

df_clean = df.copy()

df_clean.drop('customerID', axis=1, inplace=True)

df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')

df_clean['TotalCharges'].fillna(df_clean['TotalCharges'].median(), inplace=True)

df_clean['Churn'] = df_clean['Churn'].map({'Yes': 1, 'No': 0})

X = df_clean.drop('Churn', axis=1)
y = df_clean['Churn']

categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)
print("\nMissing values per column:")
print(X.isnull().sum()[X.isnull().sum() > 0])

from sklearn.preprocessing import LabelEncoder, StandardScaler

X_encoded = X.copy()

for col in categorical_cols:
    le = LabelEncoder()
    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

X_final = pd.DataFrame(X_scaled, columns=X_encoded.columns)

print("Preprocessing complete!")
print("Final feature shape:", X_final.shape)
print("Target shape:", y.shape)
print("\nSample of processed features (first 3 rows):")
print(X_final.head(3))
print("\nTarget distribution (0=Retained, 1=Churned):")
print(y.value_counts())
print("Churn rate: {:.2f}%".format((y.sum() / len(y)) * 100))

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

X_train, X_test, y_train, y_test = train_test_split(
    X_final, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Before SMOTE:")
print(f"Training set shape: {X_train.shape}")
print(f"Churn distribution in training: {y_train.value_counts().to_dict()}")
print(f"Churn rate in training: {y_train.mean()*100:.2f}%")

# Added SMOTE to handle class imbalance (critical for churn prediction)
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print("\nAfter SMOTE:")
print(f"Balanced training set shape: {X_train_balanced.shape}")
print(f"Churn distribution after SMOTE: {pd.Series(y_train_balanced).value_counts().to_dict()}")
print(f"Churn rate after SMOTE: {y_train_balanced.mean()*100:.2f}%")

from sklearn.ensemble import RandomForestClassifier

# Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train_balanced, y_train_balanced)

print("Random Forest model trained successfully!")
print(f"Number of trees: {rf_model.n_estimators}")
print(f"Max depth: {rf_model.max_depth}")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score

y_pred = rf_model.predict(X_test)
y_proba = rf_model.predict_proba(X_test)[:, 1]

print("="*60)
print("MODEL EVALUATION ON TEST SET")
print("="*60)
print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")
print(f"\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Retained (0)', 'Churned (1)']))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Retained', 'Churned'],
            yticklabels=['Retained', 'Churned'])
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.tight_layout()
plt.show()

# ROC Curve
from sklearn.metrics import roc_curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr,label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_proba):.4f})', color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', lw=2)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# feature importances from Random Forest
importances = rf_model.feature_importances_
feature_names = X_encoded.columns

# Create DataFrame and sort
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=feature_importance_df.head(15),
            x='Importance',
            y='Feature',
            palette='viridis')
plt.title('Top 15 Features Driving Customer Churn', fontsize=16, fontweight='bold')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

print("="*60)
print("TOP 10 CHURN DRIVERS")
print("="*60)
top_10 = feature_importance_df.head(10)
for idx, row in top_10.iterrows():
    print(f"{row['Feature']:30s} : {row['Importance']:.4f} ({row['Importance']*100:.2f}%)")

import joblib

# Save model and scaler for future use
joblib.dump(rf_model, 'churn_prediction_model.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')
joblib.dump(categorical_cols, 'categorical_columns.pkl')

print(" Model and preprocessing objects saved successfully!")
print("Files created:")
print("  • churn_prediction_model.pkl  → Trained Random Forest model")
print("  • feature_scaler.pkl          → Feature scaler")
print("  • categorical_columns.pkl     → List of categorical columns")

#  reusable prediction function
def predict_churn(customer_data):
    """
    Predict churn probability for a new customer

    Parameters:
    customer_data: dict with customer features (same as dataset columns except customerID)

    Returns:
    dict with churn probability and risk level
    """
    # Load saved objects
    model = joblib.load('churn_prediction_model.pkl')
    scaler_obj = joblib.load('feature_scaler.pkl')
    cat_cols = joblib.load('categorical_columns.pkl')

    #DataFrame
    df_new = pd.DataFrame([customer_data])

    # Handle TotalCharges if missing
    if 'TotalCharges' in df_new.columns and pd.isna(df_new['TotalCharges'].iloc[0]):
        df_new['TotalCharges'] = df['TotalCharges'].median()

    #categorical variables
    for col in cat_cols:
        if col in df_new.columns:
            le = LabelEncoder()
            le.fit(X[col].astype(str))  # Fit on original training data categories
            df_new[col] = le.transform(df_new[col].astype(str))

    # Scale features
    X_new_scaled = scaler_obj.transform(df_new)

    # Predict
    proba = model.predict_proba(X_new_scaled)[0][1]

    # Determine risk level
    if proba >= 0.7:
        risk = "HIGH RISK - Immediate intervention needed"
    elif proba >= 0.4:
        risk = "MEDIUM RISK - Monitor closely"
    else:
        risk = "LOW RISK - Standard engagement"

    return {
        'churn_probability': round(proba, 4),
        'risk_level': risk,
        'recommendation': risk
    }

# Test the function with a sample customer
sample_customer = {
    'gender': 'Female',
    'SeniorCitizen': 0,
    'Partner': 'Yes',
    'Dependents': 'No',
    'tenure': 5,
    'PhoneService': 'Yes',
    'MultipleLines': 'No',
    'InternetService': 'Fiber optic',
    'OnlineSecurity': 'No',
    'OnlineBackup': 'No',
    'DeviceProtection': 'No',
    'TechSupport': 'No',
    'StreamingTV': 'No',
    'StreamingMovies': 'No',
    'Contract': 'Month-to-month',
    'PaperlessBilling': 'Yes',
    'PaymentMethod': 'Electronic check',
    'MonthlyCharges': 70.0,
    'TotalCharges': 350.0
}

result = predict_churn(sample_customer)
print("\n" + "="*60)
print("SAMPLE PREDICTION FOR NEW CUSTOMER")
print("="*60)
print(f"Churn Probability: {result['churn_probability']:.2%}")
print(f"Risk Level: {result['risk_level']}")

# Generate churn risk segments for all customers
df_with_predictions = df.copy()
df_with_predictions['Churn_Probability'] = rf_model.predict_proba(X_final)[:, 1]

# Create risk segments
def get_risk_segment(proba):
    if proba >= 0.7:
        return 'HIGH RISK'
    elif proba >= 0.4:
        return 'MEDIUM RISK'
    else:
        return 'LOW RISK'

df_with_predictions['Risk_Segment'] = df_with_predictions['Churn_Probability'].apply(get_risk_segment)

# Business insights by segment
print("="*70)
print("CHURN RISK SEGMENTATION (Business-Ready Insights)")
print("="*70)
print(df_with_predictions['Risk_Segment'].value_counts())
print("\nAverage Monthly Charges by Risk Segment:")
print(df_with_predictions.groupby('Risk_Segment')['MonthlyCharges'].mean().round(2))
print("\nAverage Tenure (months) by Risk Segment:")
print(df_with_predictions.groupby('Risk_Segment')['tenure'].mean().round(2))

# Save segmented customers for action
df_with_predictions.to_csv('customer_churn_risk_segments.csv', index=False)
print("\nSaved segmented customers to 'customer_churn_risk_segments.csv'")
print("   → Use this file for targeted retention campaigns!")

# Business Recommendations Table
print("\n" + "="*70)
print("ACTIONABLE BUSINESS RECOMMENDATIONS")
print("="*70)
recommendations = pd.DataFrame({
    'Risk Segment': ['HIGH RISK', 'MEDIUM RISK', 'LOW RISK'],
    'Profile': [
        'Short tenure (<6mo), Month-to-month contract, Fiber optic internet',
        'Declining usage, No online security/support, Paperless billing',
        'Long tenure (>24mo), Annual contract, Multiple services'
    ],
    'Intervention': [
        'IMMEDIATE: Offer 3-mo discount + switch to annual contract',
        ' MONITOR: Proactive check-in call + free tech support month',
        'RETAIN: Loyalty rewards + upsell premium services'
    ],
    'Expected ROI': [
        'High (Save $500+/customer)',
        'Medium (Save $300+/customer)',
        'Low (Upsell $100+/customer)'
    ]
})
print(recommendations.to_string(index=False))

import joblib
import os

# Save artifacts
joblib.dump(rf_model, 'churn_prediction_model.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')
joblib.dump(X.columns.tolist(), 'model_features.pkl')  # Critical: saves feature order!
joblib.dump(categorical_cols, 'categorical_columns.pkl')

print("✅ Artifacts saved!")
print("Files created:")
for f in os.listdir():
    if f.endswith('.pkl'):
        print(f"  • {f}")

import zipfile
import os

# Create ZIP of all .pkl files
with zipfile.ZipFile('churn_model_artifacts.zip', 'w') as zipf:
    for file in os.listdir():
        if file.endswith('.pkl'):
            zipf.write(file)

print("✅ ZIP created: churn_model_artifacts.zip")
print("⬇️ Click the file in left sidebar → Download")